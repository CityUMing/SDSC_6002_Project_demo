{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9031260,"sourceType":"datasetVersion","datasetId":5399141}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\nimport torchaudio\nfrom datasets import Dataset\nfrom transformers import WhisperProcessor, WhisperForConditionalGeneration\n!pip install evaluate\n!pip install jiwer\nimport evaluate\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-08-01T15:36:43.710342Z","iopub.execute_input":"2024-08-01T15:36:43.711222Z","iopub.status.idle":"2024-08-01T15:37:28.126148Z","shell.execute_reply.started":"2024-08-01T15:36:43.711181Z","shell.execute_reply":"2024-08-01T15:37:28.125361Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.20.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.2)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.5.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.7.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\nCollecting jiwer\n  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\nDownloading jiwer-3.0.4-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.9.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m80.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.4 rapidfuzz-3.9.5\n","output_type":"stream"},{"name":"stderr","text":"2024-08-01 15:37:19.092293: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-01 15:37:19.092391: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-01 15:37:19.189770: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# 1. 加载数据\ndef load_audio_files(audio_dir):\n    audio_files = []\n    for filename in os.listdir(audio_dir):\n        if filename.endswith(\".wav\"):\n            audio_path = os.path.join(audio_dir, filename)\n            audio_files.append(audio_path)\n    audio_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n    return audio_files\n\ndef load_transcripts(transcript_dir):\n    transcripts = []\n    txt_files = [f for f in os.listdir(transcript_dir) if f.endswith('.txt')]\n    txt_files.sort(key=lambda x: int(os.path.splitext(x)[0]))\n    for file in txt_files:\n        with open(os.path.join(transcript_dir, file), 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read().strip()\n            transcripts.append(text)\n    return transcripts\n\ndef resample_audio(waveform, original_rate, target_rate):\n    resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=target_rate)\n    return resampler(waveform)\n\ndef create_dataset(audio_files, transcripts):\n    data = {\"audio\": [], \"sentence\": []}\n    for audio_file, transcript in zip(audio_files, transcripts):\n        waveform, sample_rate = torchaudio.load(audio_file)\n        if sample_rate != 16000:\n            waveform = resample_audio(waveform, sample_rate, 16000)\n        audio = {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": 16000}\n        data[\"audio\"].append(audio)\n        data[\"sentence\"].append(transcript)\n    return Dataset.from_dict(data)\n\naudio_dir = \"/kaggle/input/6002project/training_data\"\ntranscript_dir = \"/kaggle/input/6002project/reference\"\n\naudio_files = load_audio_files(audio_dir)\ntranscripts = load_transcripts(transcript_dir)\ndataset = create_dataset(audio_files, transcripts)\n\n# 2. 划分数据集\ntotal_size = len(dataset)\neval_size = int(0.1 * total_size)\n\neval_dataset = dataset.select(range(eval_size))\n\n# 3. 加载预训练模型和处理器\nprocessor = WhisperProcessor.from_pretrained(\"alvanlii/whisper-small-cantonese\",language=\"yue\",task=\"transcribe\")\nmodel = WhisperForConditionalGeneration.from_pretrained(\"alvanlii/whisper-small-cantonese\")\nmodel.eval()\n\n# 4. 对评估数据集进行转录\ndef transcribe_audio(batch):\n    input_features = processor.feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=batch[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\").input_features\n    input_features = input_features.to(model.device)\n    with torch.no_grad():\n        predicted_ids = model.generate(input_features)\n    transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n    return transcription\n\neval_dataset = eval_dataset.map(lambda batch: {\"prediction\": transcribe_audio(batch)}, remove_columns=[\"audio\"])\n\n# 去掉标点符号和空格\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]', '', text)  # 去掉标点符号\n    text = re.sub(r'\\s+', '', text)  # 去掉多余的空格\n    return text\n\ndef split_chars(text):\n    return \" \".join(list(text))\n\ndef tokenize_text(text):\n    cleaned_text = clean_text(text)\n    tokenized_text = split_chars(cleaned_text)\n    return tokenized_text\n\n# 5. 计算WER\npredictions = eval_dataset[\"prediction\"]\nreferences = eval_dataset[\"sentence\"]\n\n# 对预测和参考句子进行分词和清理\npredictions_tokenized = [tokenize_text(pred) for pred in predictions]\nreferences_tokenized = [tokenize_text(ref) for ref in references]\n\n# 打印分词结果以供检查\nfor pred_tok, ref_tok in zip(predictions_tokenized, references_tokenized):\n    print(f\"Tokenized Prediction: {pred_tok}\")\n    print(f\"Tokenized Reference: {ref_tok}\")\n    print(\"------\")\n\n# 使用 evaluate 计算并输出WER\nmetric = evaluate.load(\"wer\")\nwer_result = metric.compute(predictions=predictions_tokenized, references=references_tokenized)\nprint(f\"WER: {wer_result * 100:.2f}%\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-01T15:43:10.842266Z","iopub.execute_input":"2024-08-01T15:43:10.842857Z","iopub.status.idle":"2024-08-01T15:48:28.309017Z","shell.execute_reply.started":"2024-08-01T15:43:10.842817Z","shell.execute_reply":"2024-08-01T15:48:28.308141Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d03f4e97996c41dda86af2e0067975cf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/967M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e25c01e212af4879895ced871ee783b0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/275 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32fce1bb38174e0091a0633040c5b65e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c87055b10c604a8eab78815ca8760e87"}},"metadata":{}},{"name":"stdout","text":"Tokenized Prediction: 倒 消 又 重 補 都 好 再 重 好 多 補\nTokenized Reference: 讀 書 要 從 薄 到 厚 再 從 厚 到 薄\n------\nTokenized Prediction: 所 謂 會 讀 書 就 係 本 住 誠 意 虧 度 有 加 之 個 數\nTokenized Reference: 所 謂 會 讀 書 就 係 本 住 誠 意 去 讀 有 價 值 嘅 書\n------\nTokenized Prediction: 好 消 死 人 更 動 得 幸 數 人 生\nTokenized Reference: 好 書 使 人 更 懂 得 享 受 人 生\n------\nTokenized Prediction: 打 開 你 哋 有 聲 好 笑 我 哋 就 可 以 用 耳 朵 怨 冷 群 笑 聆 聽 中 外 不 同 駐 坐\nTokenized Reference: 打 開 呢 本 有 聲 好 書 我 哋 就 可 以 用 耳 朵 閱 覽 群 書 聆 聽 中 外 不 同 著 作\n------\nTokenized Prediction: 香 港 電 台 J Z W O\nTokenized Reference: 香 港 電 臺 製 作\n------\nTokenized Prediction: 有 聲 好 笑\nTokenized Reference: 有 聲 好 書\n------\nTokenized Prediction: 需 置 痛 癮 啲 嘈 請 老 好 故 事\nTokenized Reference: 最 折 騰 人 的 籌 錢 留 學 故 事\n------\nTokenized Prediction: 為 咗 掃 請 六 號 以 利 卡 加 盞 其 中 最 令 人 登 湖 詐 詩 嘅 女 子 係 呢 時 好 家 李 東 風\nTokenized Reference: 為 咗 索 錢 留 學 而 累 及 家 人 其 中 最 令 人 燈 目 咋 舌 嘅 例 子 係 歷 史 學 家 黎 東 方\n------\nTokenized Prediction: 佢 令 一 家 人 連 膨 脂 親 友 都 兩 作 一 團\nTokenized Reference: 佢 令 一 家 人 連 旁 支 親 友 都 亂 作 一 團\n------\nTokenized Prediction: 仲 姦 字 好 死 祖 苦 親\nTokenized Reference: 仲 間 接 害 死 咗 父 親\n------\nTokenized Prediction: 請 因 為 父 母 嘅 呢 嗌\nTokenized Reference: 全 因 為 父 母 嘅 溺 愛\n------\nTokenized Prediction: 呢 個 望 母 老 昆 嘅 友 姐 先 至 多 誰 所 願 係 一 個 二 百 年 去 發 過 落 後\nTokenized Reference: 呢 個 盲 目 樂 觀 嘅 幼 子 先 至 得 誰 所 願 喺 一 九 二 八 年 去 法 國 留 學\n------\nTokenized Prediction: 東 市 二 十 一 歲 嘅 李 東 方 喺 清 華 大 學 已 經 讀 到 新 年 卡\nTokenized Reference: 當 時 二 十 一 歲 嘅 黎 東 方 喺 清 華 大 學 已 經 讀 到 三 年 級\n------\nTokenized Prediction: 九 四 清 華 已 經 嗌 新 齋\nTokenized Reference: 嗰 時 清 華 已 經 係 新 制\n------\nTokenized Prediction: 再 沒 有 自 動 老 美 一 回 事\nTokenized Reference: 再 沒 有 自 動 留 美 一 回 事\n------\nTokenized Prediction: 但 係 仲 係 萬 千 人 期 望 約 到 個 大 號\nTokenized Reference: 但 係 仍 係 萬 千 人 期 望 入 讀 嘅 大 學\n------\nTokenized Prediction: 李 東 方 聽 人 講 巴 黎 大 號 唔 講 資 格 只 講 號 利\nTokenized Reference: 黎 東 方 聽 人 講 巴 黎 大 學 唔 講 資 格 只 講 學 歷\n------\nTokenized Prediction: 任 何 人 可 以 直 接 後 過 視\nTokenized Reference: 任 何 人 可 以 直 接 考 博 士\n------\nWER: 42.53%\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq\n# 1. 加载数据\ndef load_audio_files(audio_dir):\n    audio_files = []\n    for filename in os.listdir(audio_dir):\n        if filename.endswith(\".wav\"):\n            audio_path = os.path.join(audio_dir, filename)\n            audio_files.append(audio_path)\n    audio_files.sort(key=lambda x: int(os.path.splitext(os.path.basename(x))[0]))\n    return audio_files\n\ndef load_transcripts(transcript_dir):\n    transcripts = []\n    txt_files = [f for f in os.listdir(transcript_dir) if f.endswith('.txt')]\n    txt_files.sort(key=lambda x: int(os.path.splitext(x)[0]))\n    for file in txt_files:\n        with open(os.path.join(transcript_dir, file), 'r', encoding='utf-8', errors='ignore') as f:\n            text = f.read().strip()\n            transcripts.append(text)\n    return transcripts\n\ndef resample_audio(waveform, original_rate, target_rate):\n    resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=target_rate)\n    return resampler(waveform)\n\ndef create_dataset(audio_files, transcripts):\n    data = {\"audio\": [], \"sentence\": []}\n    for audio_file, transcript in zip(audio_files, transcripts):\n        waveform, sample_rate = torchaudio.load(audio_file)\n        if sample_rate != 16000:\n            waveform = resample_audio(waveform, sample_rate, 16000)\n        audio = {\"array\": waveform.squeeze().numpy(), \"sampling_rate\": 16000}\n        data[\"audio\"].append(audio)\n        data[\"sentence\"].append(transcript)\n    return Dataset.from_dict(data)\n\naudio_dir = \"/kaggle/input/6002project/training_data\"\ntranscript_dir = \"/kaggle/input/6002project/reference\"\n\naudio_files = load_audio_files(audio_dir)\ntranscripts = load_transcripts(transcript_dir)\ndataset = create_dataset(audio_files, transcripts)\n\n# 2. 划分数据集\ntotal_size = len(dataset)\neval_size = int(0.1 * total_size)\n\neval_dataset = dataset.select(range(eval_size))\n\n# 3. 加载预训练模型和处理器\n#processor = WhisperProcessor.from_pretrained(\"Scrya/whisper-large-v2-cantonese\",language=\"yue\",task=\"transcribe\")\n#model = WhisperForConditionalGeneration.from_pretrained(\"Scrya/whisper-large-v2-cantonese\")\nprocessor = AutoProcessor.from_pretrained(\"Scrya/whisper-large-v2-cantonese\")\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(\"Scrya/whisper-large-v2-cantonese\")\nmodel.eval()\n\n# 4. 对评估数据集进行转录\ndef transcribe_audio(batch):\n    input_features = processor.feature_extractor(batch[\"audio\"][\"array\"], sampling_rate=batch[\"audio\"][\"sampling_rate\"], return_tensors=\"pt\").input_features\n    input_features = input_features.to(model.device)\n    with torch.no_grad():\n        predicted_ids = model.generate(input_features)\n    transcription = processor.tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n    return transcription\n\neval_dataset = eval_dataset.map(lambda batch: {\"prediction\": transcribe_audio(batch)}, remove_columns=[\"audio\"])\n\n# 去掉标点符号和空格\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]', '', text)  # 去掉标点符号\n    text = re.sub(r'\\s+', '', text)  # 去掉多余的空格\n    return text\n\ndef split_chars(text):\n    return \" \".join(list(text))\n\ndef tokenize_text(text):\n    cleaned_text = clean_text(text)\n    tokenized_text = split_chars(cleaned_text)\n    return tokenized_text\n\n# 5. 计算WER\npredictions = eval_dataset[\"prediction\"]\nreferences = eval_dataset[\"sentence\"]\n\n# 对预测和参考句子进行分词和清理\npredictions_tokenized = [tokenize_text(pred) for pred in predictions]\nreferences_tokenized = [tokenize_text(ref) for ref in references]\n\n# 打印分词结果以供检查\nfor pred_tok, ref_tok in zip(predictions_tokenized, references_tokenized):\n    print(f\"Tokenized Prediction: {pred_tok}\")\n    print(f\"Tokenized Reference: {ref_tok}\")\n    print(\"------\")\n\n# 使用 evaluate 计算并输出WER\nmetric = evaluate.load(\"wer\")\nwer_result = metric.compute(predictions=predictions_tokenized, references=references_tokenized)\nprint(f\"WER: {wer_result * 100:.2f}%\")","metadata":{"execution":{"iopub.status.busy":"2024-08-01T16:04:54.034214Z","iopub.execute_input":"2024-08-01T16:04:54.035084Z","iopub.status.idle":"2024-08-01T17:26:55.808464Z","shell.execute_reply.started":"2024-08-01T16:04:54.035050Z","shell.execute_reply":"2024-08-01T17:26:55.807418Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/185k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b90c6cb08e4645a6db57def6e49fe2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/832 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7cacf49775a8474e8f9551408fde87fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"572cff8f58d94e7198cc30f1b0d40e3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/494k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dfdb02941d6440ea39b1075c3b273aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"normalizer.json:   0%|          | 0.00/52.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1c22363e4c04468b7be056c752034a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/2.11k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4416d8ac04994bcea4fd5f1297ae9427"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83860dd079a64c398efe6860fcf4242c"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7307f3e51f534cba9907e3e5a6fea6e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/6.17G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"61c30e5ceafd4304a00bb9a9855af5c5"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/18 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cb01f0e489e42129b579df68e5ab9cf"}},"metadata":{}},{"name":"stdout","text":"Tokenized Prediction: 屌 銷 要 重 寶 都 好 再 重 好 多 寶\nTokenized Reference: 讀 書 要 從 薄 到 厚 再 從 厚 到 薄\n------\nTokenized Prediction: 所 謂 未 讀 書 就 係 本 著 心 意 會 讀 有 加 字 嘅 書\nTokenized Reference: 所 謂 會 讀 書 就 係 本 住 誠 意 去 讀 有 價 值 嘅 書\n------\nTokenized Prediction: 好 少 死 人 更 動 得 享 受 人 生\nTokenized Reference: 好 書 使 人 更 懂 得 享 受 人 生\n------\nTokenized Prediction: 打 開 你 本 有 聲 好 笑 我 哋 就 可 以 用 已 多 元 朗 群 語 聆 聽 中 外 不 同 住 住\nTokenized Reference: 打 開 呢 本 有 聲 好 書 我 哋 就 可 以 用 耳 朵 閱 覽 群 書 聆 聽 中 外 不 同 著 作\n------\nTokenized Prediction: 香 港 電 台 在 做\nTokenized Reference: 香 港 電 臺 製 作\n------\nTokenized Prediction: 有 聲 好 笑\nTokenized Reference: 有 聲 好 書\n------\nTokenized Prediction: 最 悸 痛 人 嘅 惆 情 留 後 故 事\nTokenized Reference: 最 折 騰 人 的 籌 錢 留 學 故 事\n------\nTokenized Prediction: 為 咗 熟 悉 老 號 以 利 卡 加 人 其 中 最 令 人 登 模 渣 事 嘅 例 子 係 呢 時 好 家 李 東 峰\nTokenized Reference: 為 咗 索 錢 留 學 而 累 及 家 人 其 中 最 令 人 燈 目 咋 舌 嘅 例 子 係 歷 史 學 家 黎 東 方\n------\nTokenized Prediction: 可 憐 一 家 人 連 胖 子 親 友 都 亂 咗 一 頹\nTokenized Reference: 佢 令 一 家 人 連 旁 支 親 友 都 亂 作 一 團\n------\nTokenized Prediction: 仲 奸 賊 好 死 周 苦 親\nTokenized Reference: 仲 間 接 害 死 咗 父 親\n------\nTokenized Prediction: 串 英 文 父 母 嘅 內 愛\nTokenized Reference: 全 因 為 父 母 嘅 溺 愛\n------\nTokenized Prediction: 呢 個 盲 目 留 軍 嘅 友 姐 先 至 多 誰 所 願 係 一 嚿 一 八 年 去 法 國 留 學\nTokenized Reference: 呢 個 盲 目 樂 觀 嘅 幼 子 先 至 得 誰 所 願 喺 一 九 二 八 年 去 法 國 留 學\n------\nTokenized Prediction: 當 時 二 十 一 歲 嘅 李 東 方 喺 清 華 大 學 已 經 讀 到 三 年 卡\nTokenized Reference: 當 時 二 十 一 歲 嘅 黎 東 方 喺 清 華 大 學 已 經 讀 到 三 年 級\n------\nTokenized Prediction: 嗰 四 千 話 已 經 太 新 滯\nTokenized Reference: 嗰 時 清 華 已 經 係 新 制\n------\nTokenized Prediction: 再 問 有 自 動 老 尾 一 回 事\nTokenized Reference: 再 沒 有 自 動 留 美 一 回 事\n------\nTokenized Prediction: 但 係 仍 係 萬 千 人 期 望 要 讀 個 大 學\nTokenized Reference: 但 係 仍 係 萬 千 人 期 望 入 讀 嘅 大 學\n------\nTokenized Prediction: 李 東 方 聽 人 講 巴 黎 大 號 唔 講 資 格 只 講 號 嚟\nTokenized Reference: 黎 東 方 聽 人 講 巴 黎 大 學 唔 講 資 格 只 講 學 歷\n------\nTokenized Prediction: 任 何 人 可 以 直 接 號 破 事\nTokenized Reference: 任 何 人 可 以 直 接 考 博 士\n------\nWER: 37.16%\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}